v4:
シード値の影響はREINFORCEにより低減された．
そこでREINFORCEにlogを使ってみたが，まだ弱体化しただけ．
パラメタの調整を行って再度検証する．
正則化を行えば改善しそうだが，面倒．


v3:
mc法との相性がよく，性能の向上が確認できた．
L1,L2正則化はスクラッチ実装が大変なので省略した．
初期値がランダムなことを考えれば，良い性能をたたきだしている...のか?
改善はあんまりない．ただし，悪化も極端ではない．正直シード値の影響が大きすぎる．
対戦相手を変え続ける予定を考えると，線形関数近似のままで別にいい...かも．

正則化を使ってみるなら，
スキットラーンやらtfやらを使うのも一つ(ついでにGPU)
ただし，本質的でない部分に時間を費やしそう．


v2:
https://tadaoyamaoka.hatenablog.com/entry/2017/04/02/235014
ボナンザはロジスティック関数を使っていた．という話...
あとL1正則化も使ってたらしいよ．
重みが大量にあるから，L1正則化が有力なのかも
r=0.9付近では有力な可能性があるから，確認してみよう!



v1.
なぜかわからなかったが，強化学習においてロジスティック回帰を用いる方法は見かけない．
そして，実際にやってみてもなぜか線形関数近似の方が良い性能を出す．

一方で，将棋の評価関数の値は無限大から負の無限大までを取り，なんか，ロジスティック回帰っぽい．
どうやってるんやろう?
そっちの方が最終的な精度は良さそう．
重みの更新方法について調査する必要性もあるかもね...



ともかく，
ロジスティック関数を活性化関数っぽく使ってみたが
線形関数近似に劣る性能しか出せていない．
ロジスティック関数をはめてみるだけというのは効果がなさそう．
